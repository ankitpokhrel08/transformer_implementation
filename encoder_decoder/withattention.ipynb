{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.6036 - loss: 4.6900 - val_accuracy: 0.8236 - val_loss: 1.6107\n",
      "Epoch 2/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8059 - loss: 1.5804 - val_accuracy: 0.8278 - val_loss: 1.3524\n",
      "Epoch 3/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8080 - loss: 1.3227 - val_accuracy: 0.8250 - val_loss: 1.2373\n",
      "Epoch 4/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8123 - loss: 1.2142 - val_accuracy: 0.8307 - val_loss: 1.2111\n",
      "Epoch 5/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8147 - loss: 1.2191 - val_accuracy: 0.8302 - val_loss: 1.1917\n",
      "Epoch 6/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8171 - loss: 1.1985 - val_accuracy: 0.8331 - val_loss: 1.1889\n",
      "Epoch 7/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8173 - loss: 1.1935 - val_accuracy: 0.8338 - val_loss: 1.1836\n",
      "Epoch 8/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8182 - loss: 1.1875 - val_accuracy: 0.8340 - val_loss: 1.1785\n",
      "Epoch 9/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8090 - loss: 1.2414 - val_accuracy: 0.8338 - val_loss: 1.1762\n",
      "Epoch 10/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8153 - loss: 1.1974 - val_accuracy: 0.8338 - val_loss: 1.1863\n",
      "Epoch 11/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8201 - loss: 1.1595 - val_accuracy: 0.8340 - val_loss: 1.1908\n",
      "Epoch 12/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.8160 - loss: 1.1786 - val_accuracy: 0.8339 - val_loss: 1.1938\n",
      "Epoch 13/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.8205 - loss: 1.1425 - val_accuracy: 0.8340 - val_loss: 1.2013\n",
      "Epoch 14/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.8166 - loss: 1.1647 - val_accuracy: 0.8339 - val_loss: 1.2070\n",
      "Epoch 15/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8176 - loss: 1.1555 - val_accuracy: 0.8341 - val_loss: 1.2067\n",
      "Epoch 16/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8158 - loss: 1.1619 - val_accuracy: 0.8342 - val_loss: 1.2127\n",
      "Epoch 17/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8183 - loss: 1.1440 - val_accuracy: 0.8348 - val_loss: 1.2185\n",
      "Epoch 18/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.8205 - loss: 1.1265 - val_accuracy: 0.8355 - val_loss: 1.2232\n",
      "Epoch 19/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8173 - loss: 1.1488 - val_accuracy: 0.8362 - val_loss: 1.2254\n",
      "Epoch 20/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.8195 - loss: 1.1338 - val_accuracy: 0.8373 - val_loss: 1.2321\n",
      "Epoch 21/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8266 - loss: 1.0922 - val_accuracy: 0.8375 - val_loss: 1.2290\n",
      "Epoch 22/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.8254 - loss: 1.0909 - val_accuracy: 0.8391 - val_loss: 1.2279\n",
      "Epoch 23/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.8270 - loss: 1.0694 - val_accuracy: 0.8410 - val_loss: 1.2251\n",
      "Epoch 24/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.8301 - loss: 1.0377 - val_accuracy: 0.8413 - val_loss: 1.2213\n",
      "Epoch 25/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8257 - loss: 1.0538 - val_accuracy: 0.8418 - val_loss: 1.2279\n",
      "Epoch 26/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8280 - loss: 1.0303 - val_accuracy: 0.8430 - val_loss: 1.2262\n",
      "Epoch 27/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8301 - loss: 1.0074 - val_accuracy: 0.8427 - val_loss: 1.2324\n",
      "Epoch 28/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step - accuracy: 0.8301 - loss: 0.9967 - val_accuracy: 0.8417 - val_loss: 1.2473\n",
      "Epoch 29/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8309 - loss: 0.9785 - val_accuracy: 0.8420 - val_loss: 1.2449\n",
      "Epoch 30/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8373 - loss: 0.9374 - val_accuracy: 0.8424 - val_loss: 1.2496\n",
      "Epoch 31/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8345 - loss: 0.9376 - val_accuracy: 0.8423 - val_loss: 1.2516\n",
      "Epoch 32/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8356 - loss: 0.9213 - val_accuracy: 0.8423 - val_loss: 1.2657\n",
      "Epoch 33/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8316 - loss: 0.9334 - val_accuracy: 0.8423 - val_loss: 1.2706\n",
      "Epoch 34/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8372 - loss: 0.8952 - val_accuracy: 0.8429 - val_loss: 1.2796\n",
      "Epoch 35/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8382 - loss: 0.8727 - val_accuracy: 0.8421 - val_loss: 1.2935\n",
      "Epoch 36/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8412 - loss: 0.8511 - val_accuracy: 0.8427 - val_loss: 1.2930\n",
      "Epoch 37/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8410 - loss: 0.8405 - val_accuracy: 0.8409 - val_loss: 1.3049\n",
      "Epoch 38/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8461 - loss: 0.8083 - val_accuracy: 0.8418 - val_loss: 1.3058\n",
      "Epoch 39/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8440 - loss: 0.8066 - val_accuracy: 0.8388 - val_loss: 1.3276\n",
      "Epoch 40/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8484 - loss: 0.7770 - val_accuracy: 0.8394 - val_loss: 1.3410\n",
      "Epoch 41/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8459 - loss: 0.7800 - val_accuracy: 0.8389 - val_loss: 1.3443\n",
      "Epoch 42/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8507 - loss: 0.7476 - val_accuracy: 0.8394 - val_loss: 1.3513\n",
      "Epoch 43/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8512 - loss: 0.7375 - val_accuracy: 0.8384 - val_loss: 1.3640\n",
      "Epoch 44/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - accuracy: 0.8517 - loss: 0.7339 - val_accuracy: 0.8381 - val_loss: 1.3674\n",
      "Epoch 45/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8543 - loss: 0.7188 - val_accuracy: 0.8377 - val_loss: 1.3875\n",
      "Epoch 46/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8585 - loss: 0.6912 - val_accuracy: 0.8394 - val_loss: 1.3827\n",
      "Epoch 47/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8599 - loss: 0.6776 - val_accuracy: 0.8380 - val_loss: 1.3985\n",
      "Epoch 48/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8595 - loss: 0.6743 - val_accuracy: 0.8380 - val_loss: 1.4098\n",
      "Epoch 49/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8655 - loss: 0.6479 - val_accuracy: 0.8371 - val_loss: 1.4182\n",
      "Epoch 50/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.8689 - loss: 0.6214 - val_accuracy: 0.8378 - val_loss: 1.4270\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Example Translation\u001b[39;00m\n\u001b[1;32m    115\u001b[0m example_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mधर्मक्षेत्रे कुरुक्षेत्रे\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtranslate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_sentence\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[59], line 96\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     93\u001b[0m input_padded \u001b[38;5;241m=\u001b[39m pad_sequences(input_seq, maxlen\u001b[38;5;241m=\u001b[39mmax_length_sanskrit, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 96\u001b[0m decoder_input[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_english\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     98\u001b[0m translated_sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length_english):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Bidirectional, AdditiveAttention\n",
    "\n",
    "# Load and preprocess dataset\n",
    "with open(\"data/geeta.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "sanskrit_lines = lines[0::2]  # Sanskrit in even indexes\n",
    "english_lines = lines[1::2]   # English in odd indexes\n",
    "\n",
    "# Ensure both lists have the same length\n",
    "min_length = min(len(sanskrit_lines), len(english_lines))\n",
    "sanskrit_lines = sanskrit_lines[:min_length]\n",
    "english_lines = english_lines[:min_length]\n",
    "\n",
    "# Add start and end tokens\n",
    "def add_tokens(text):\n",
    "    return 'start_ ' + text.lower() + ' _end'\n",
    "\n",
    "english_lines = [add_tokens(sent) for sent in english_lines]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer_sanskrit = Tokenizer()\n",
    "tokenizer_english = Tokenizer()\n",
    "\n",
    "tokenizer_sanskrit.fit_on_texts(sanskrit_lines)\n",
    "tokenizer_english.fit_on_texts(english_lines)\n",
    "\n",
    "sanskrit_sequences = tokenizer_sanskrit.texts_to_sequences(sanskrit_lines)\n",
    "english_sequences = tokenizer_english.texts_to_sequences(english_lines)\n",
    "\n",
    "# Padding\n",
    "max_length_sanskrit = max(len(seq) for seq in sanskrit_sequences)\n",
    "max_length_english = max(len(seq) for seq in english_sequences)\n",
    "\n",
    "sanskrit_padded = pad_sequences(sanskrit_sequences, maxlen=max_length_sanskrit, padding='post')\n",
    "english_padded = pad_sequences(english_sequences, maxlen=max_length_english, padding='post')\n",
    "\n",
    "# Prepare decoder input and target data\n",
    "decoder_input_data = english_padded[:, :-1]  # Remove last token\n",
    "decoder_target_data = english_padded[:, 1:]  # Remove first token\n",
    "\n",
    "# Model Parameters\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_length_sanskrit,))\n",
    "enc_embedding = Embedding(input_dim=len(tokenizer_sanskrit.word_index)+1, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(enc_embedding)\n",
    "\n",
    "state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_embedding = Embedding(input_dim=len(tokenizer_english.word_index)+1, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units * 2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Attention Layer\n",
    "attention = AdditiveAttention()\n",
    "attention_result = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_combined_context = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attention_result])\n",
    "\n",
    "decoder_dense = Dense(len(tokenizer_english.word_index)+1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Define the Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "model.fit(\n",
    "    [sanskrit_padded, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: others offer few age age achieve achieve difficulties restraint destroyed degrade honour and pain and humility and humility and egoism achieved achieved achieved achieved proper expedients expedients in the correct conclusion of the effulgence of the rik saman and yajus yajus self\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(input_text):\n",
    "    # Convert the input text to sequence and pad it\n",
    "    input_seq = tokenizer_sanskrit.texts_to_sequences([input_text])\n",
    "    input_padded = pad_sequences(input_seq, maxlen=max_length_sanskrit, padding='post')\n",
    "\n",
    "    # Initialize the decoder input with the correct start token ('start' not 'start_')\n",
    "    decoder_input = np.array([[tokenizer_english.word_index['start']]])\n",
    "    translated_sentence = []\n",
    "\n",
    "    # Generate tokens one by one up to the maximum length\n",
    "    for _ in range(max_length_english):\n",
    "        predictions = model.predict([input_padded, decoder_input], verbose=0)\n",
    "        predicted_id = np.argmax(predictions[0, -1, :])\n",
    "\n",
    "        # Break if the predicted token is the end token ('end' not '_end')\n",
    "        if predicted_id == tokenizer_english.word_index['end']:\n",
    "            break\n",
    "\n",
    "        # Retrieve the predicted word\n",
    "        predicted_word = tokenizer_english.index_word.get(predicted_id, '')\n",
    "        translated_sentence.append(predicted_word)\n",
    "\n",
    "        # Append the predicted token to the decoder input\n",
    "        decoder_input = np.concatenate([decoder_input, np.array([[predicted_id]])], axis=1)\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: others offer few age age achieve achieve difficulties restraint destroyed degrade honour and pain and humility and humility and egoism achieved achieved achieved achieved proper expedients expedients in the correct conclusion of the effulgence of the rik saman and yajus yajus self\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Translation\n",
    "example_sentence = \"धर्मक्षेत्रे कुरुक्षेत्रे समवेता युयुत्सवः\"\n",
    "print(\"Translated:\", translate_sentence(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
