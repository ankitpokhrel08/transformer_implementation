# Sanskrit-to-English Neural Machine Translation ЁЯХЙя╕П

A character-level Transformer model for translating English text to Sanskrit using PyTorch, with a beautiful Streamlit web interface.

## ЁЯОп Overview

This project implements a complete neural machine translation system that converts English text to Sanskrit (Devanagari script) using a Transformer architecture with multi-head attention mechanisms.

## тЬи Features

- **ЁЯза Transformer Architecture**: Full encoder-decoder with multi-head attention
- **ЁЯУЭ Character-Level Translation**: Fine-grained tokenization for better accuracy
- **я┐╜ Complete Vocabulary**: Comprehensive Sanskrit character set 

## ЁЯЪА Quick Start

### 1. **Install Dependencies**

```bash
pip install torch numpy streamlit matplotlib jupyter
```

### 2. **Train the Model** (Optional)

```bash
cd transformer/
jupyter notebook final_transformer.ipynb
# Run all cells to train from scratch
```

### 3. **Launch Web App**

```bash
streamlit run app.py
```

### 4. **Use the Model**

- Open `http://localhost:8501` in your browser
- Enter English text: _"Your right is to perform your duty only"_
- Get Sanskrit output: _рдХрд░реНрдордгреНрдпреЗрд╡рд╛рдзрд┐рдХрд╛рд░рд╕реНрддреЗ рдорд╛ рдлрд▓реЗрд╖реБ рдХрджрд╛рдЪрди_

## ЁЯза Model Architecture

- **Type**: Encoder-Decoder Transformer
- **Dimensions**: 512 (d_model), 2048 (FFN)
- **Attention Heads**: 8
- **Layers**: 1 (configurable)
- **Vocabulary**: 89 Sanskrit + 183 English characters
- **Max Length**: 200 characters
- **Training**: Adam optimizer, Cross-entropy loss


### Sample Translations:

- _"I am here"_ тЖТ _рдЕрд╣рдореН рдЕрддреНрд░ рдЕрд╕реНрдорд┐_
- _"Do work don't expect result"_ тЖТ _рдХрд░реНрдо рдХреБрд░реНрд╡рдиреНрддреБ рдлрд▓рдВ рдорд╛ рдкреНрд░рддреНрдпрд╛рд╢рдпрдиреНрддреБ_



## ЁЯФз Technical Details

- **Framework**: PyTorch 2.0+
- **Training Data**: English-Sanskrit parallel corpus
- **Tokenization**: Character-level with special tokens
- **Attention**: Multi-head self-attention + cross-attention
- **Masking**: Look-ahead and padding masks

## я┐╜ Training Process

1. **Data Preparation**: Filter valid sentence pairs
2. **Vocabulary Building**: Character-level tokenization
3. **Model Initialization**: Xavier uniform weights
4. **Training Loop**: 50 epoch with checkpoint saving
5. **Evaluation**: Real-time translation testing

## ЁЯОп Performance

- **Training**: ~50 epochs on parallel corpus
- **Inference**: Real-time character generation
- **Accuracy**: Not accurate

