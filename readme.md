# Sanskrit-to-English Neural Machine Translation ğŸ•‰ï¸

A character-level Transformer model for translating English text to Sanskrit using PyTorch, with a beautiful Streamlit web interface.

## ğŸ¯ Overview

This project implements a complete neural machine translation system that converts English text to Sanskrit (Devanagari script) using a Transformer architecture with multi-head attention mechanisms.

## âœ¨ Features

- **ğŸ§  Transformer Architecture**: Full encoder-decoder with multi-head attention
- **ğŸ“ Character-Level Translation**: Fine-grained tokenization for better accuracy
- **ğŸŒ Web Interface**: Dark-themed Streamlit app for real-time testing
- **ï¿½ Complete Vocabulary**: Comprehensive Sanskrit character set with IAST support
- **âš¡ GPU/MPS Support**: Optimized for CUDA and Apple Silicon (M1/M2/M3)
- **ğŸ’¿ Model Persistence**: Checkpoint saving and vocabulary export

## ğŸ—ï¸ Project Structure

```
sanskrit_to_english/
â”œâ”€â”€ ğŸ“Š data/
â”‚   â”œâ”€â”€ dev.en                    # English sentences
â”‚   â”œâ”€â”€ dev.sn                    # Sanskrit sentences
â”‚   â””â”€â”€ cleaning.ipynb            # Data preprocessing
â”œâ”€â”€ ğŸ¤– transformer/
â”‚   â”œâ”€â”€ transformer.py            # Main model implementation
â”‚   â”œâ”€â”€ final_transformer.ipynb   # Training notebook
â”‚   â”œâ”€â”€ working_transformer.ipynb # Development notebook
â”‚   â””â”€â”€ checkpoint.pth            # Trained model weights
â”œâ”€â”€ ğŸ“¦ models/
â”‚   â””â”€â”€ sanskrit_vocabulary.pkl   # Exported vocabulary data
â”œâ”€â”€ ğŸŒ app.py                     # Streamlit web interface
â”œâ”€â”€ ğŸ“‹ requirements.txt           # Python dependencies
â””â”€â”€ ğŸš€ run_app.sh                # Quick start script
```

## ğŸš€ Quick Start

### 1. **Install Dependencies**

```bash
pip install torch numpy streamlit matplotlib jupyter
```

### 2. **Train the Model** (Optional)

```bash
cd transformer/
jupyter notebook final_transformer.ipynb
# Run all cells to train from scratch
```

### 3. **Launch Web App**

```bash
chmod +x run_app.sh
./run_app.sh
```

### 4. **Use the Model**

- Open `http://localhost:8501` in your browser
- Enter English text: _"Your right is to perform your duty only"_
- Get Sanskrit output: _à¤•à¤°à¥à¤®à¤£à¥à¤¯à¥‡à¤µà¤¾à¤§à¤¿à¤•à¤¾à¤°à¤¸à¥à¤¤à¥‡ à¤®à¤¾ à¤«à¤²à¥‡à¤·à¥ à¤•à¤¦à¤¾à¤šà¤¨_

## ğŸ§  Model Architecture

- **Type**: Encoder-Decoder Transformer
- **Dimensions**: 512 (d_model), 2048 (FFN)
- **Attention Heads**: 8
- **Layers**: 1 (configurable)
- **Vocabulary**: 89 Sanskrit + 183 English characters
- **Max Length**: 200 characters
- **Training**: Adam optimizer, Cross-entropy loss

## ğŸ“Š Vocabulary Coverage

### Sanskrit (89 tokens):

- **Vowels**: à¤… à¤† à¤‡ à¤ˆ à¤‰ à¤Š à¤‹ à¥  à¤Œ à¥¡ à¤ à¤ à¤“ à¤”
- **Consonants**: All 33 Devanagari consonants
- **Diacritics**: à¤¾ à¤¿ à¥€ à¥ à¥‚ à¥ƒ à¥„ à¥‡ à¥ˆ à¥‹ à¥Œ
- **Special**: à¤‚ à¤ƒ à¤ à¥ à¥¤ à¥¥ (anusvara, visarga, virama, etc.)

### English (183 tokens):

- Standard ASCII + Extended Latin + IAST transliteration
- Unicode support for scholarly texts

## ğŸŒ Web Interface

The Streamlit app provides:

- **ğŸŒ‘ Dark Theme**: Elegant black background with Sanskrit colors
- **âš¡ Real-time Translation**: Character-by-character generation
- **ğŸ“Š Model Info**: Vocabulary sizes, parameters, device status
- **ğŸ“– Examples**: Built-in test sentences
- **ğŸ“‹ Copy Support**: Easy copying of Sanskrit output

### Sample Translations:

- _"I am here"_ â†’ _à¤…à¤¹à¤®à¥ à¤…à¤¤à¥à¤° à¤…à¤¸à¥à¤®à¤¿_
- _"Do work don't expect result"_ â†’ _à¤•à¤°à¥à¤® à¤•à¥à¤°à¥à¤µà¤¨à¥à¤¤à¥ à¤«à¤²à¤‚ à¤®à¤¾ à¤ªà¥à¤°à¤¤à¥à¤¯à¤¾à¤¶à¤¯à¤¨à¥à¤¤à¥_

## ğŸ’» Usage Examples

### Command Line (Python):

```python
from transformer import Transformer
import pickle

# Load vocabulary
with open('models/sanskrit_vocabulary.pkl', 'rb') as f:
    vocab_data = pickle.load(f)

# Initialize and load model
transformer = Transformer(...)  # with vocab_data parameters
checkpoint = torch.load('transformer/checkpoint.pth')
transformer.load_state_dict(checkpoint['model_state_dict'])

# Translate
result = translate("Your text here")
print(result)  # Sanskrit output
```

### Web Interface:

```bash
streamlit run app.py
```

## ğŸ”§ Technical Details

- **Framework**: PyTorch 2.0+
- **Training Data**: English-Sanskrit parallel corpus
- **Tokenization**: Character-level with special tokens
- **Attention**: Multi-head self-attention + cross-attention
- **Masking**: Look-ahead and padding masks
- **Device Support**: CUDA, MPS (Apple Silicon), CPU

## ï¿½ Training Process

1. **Data Preparation**: Filter valid sentence pairs
2. **Vocabulary Building**: Character-level tokenization
3. **Model Initialization**: Xavier uniform weights
4. **Training Loop**: 50+ epochs with checkpoint saving
5. **Evaluation**: Real-time translation testing

## ğŸ¯ Performance

- **Training**: ~50 epochs on parallel corpus
- **Inference**: Real-time character generation
- **Memory**: Efficient checkpoint system
- **Accuracy**: Contextual Sanskrit generation

## ï¿½ï¸ Customization

### Model Parameters:

```python
d_model = 512        # Model dimension
num_heads = 8        # Attention heads
num_layers = 1       # Transformer layers
max_length = 200     # Sequence length
```

### Training Configuration:

```python
batch_size = 30      # Training batch size
learning_rate = 1e-4 # Adam learning rate
epochs = 50          # Training epochs
```

## ğŸ› Troubleshooting

### Common Issues:

- **Model not loading**: Check `checkpoint.pth` exists
- **Vocabulary error**: Ensure `sanskrit_vocabulary.pkl` is present
- **Import errors**: Install PyTorch and dependencies
- **CUDA/MPS**: Model auto-detects available device

### File Verification:

```bash
ls transformer/checkpoint.pth models/sanskrit_vocabulary.pkl
```

## ğŸ” Key Insights & Limitations

### Current Approach:

- **Character-level tokenization**: Simple but limited for morphologically rich Sanskrit
- **Single-layer architecture**: Fast training but limited capacity
- **Real-time inference**: Good for demonstration and testing

### Potential Improvements:

- **Subword tokenization** (BPE/SentencePiece) for better semantic units
- **Deeper models** (6+ layers) for increased capacity
- **Pre-trained multilingual models** (mBERT, IndicBERT) for better performance
- **Larger datasets** for improved generalization

## ğŸ“ Notes

- **Character-level**: Works with any input length
- **Sanskrit Script**: Outputs proper Devanagari
- **Extensible**: Easy to add more languages/features
- **Research**: Based on "Attention is All You Need" paper
- **Educational**: Great for understanding Transformer architecture

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Add improvements
4. Test with various inputs
5. Submit pull request

## ğŸ“„ License

Open source - feel free to use for research and education.

---

**ğŸ•‰ï¸ Preserving Ancient Wisdom Through Modern Technology**

_Built with PyTorch, Streamlit, and dedication to Sanskrit literature_
